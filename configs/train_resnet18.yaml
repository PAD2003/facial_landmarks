# @package _global_

# specify here default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - _self_
  - callbacks: default.yaml

data:
  _target_: src.data.dlib_datamodule.DlibDataModule

  defaults:
    - transform_train: cutout.yaml
    - transform_val: default.yaml
    - _self_

  data_train: 
    _target_: src.data.components.dlib_dataset.DlibDataset
    _partial_: true
    xml_file: labels_ibug_300W_train.xml

  data_test:
    _target_: src.data.components.dlib_dataset.DlibDataset
    _partial_: true
    xml_file: labels_ibug_300W_test.xml

  # data_dir: data/ibug_300W_large_face_landmark_dataset
  data_dir: ${paths.data_dir}/ibug_300W_large_face_landmark_dataset

  train_val_test_split: [5666, 1000]

  batch_size: 16

  num_workers: 2

  pin_memory: false

model:
  _target_: src.models.dlib_module.DlibModule

  net:
    _target_: src.models.components.simple_resnet.SimpleResnet
    model_name: resnet18
    weights: DEFAULT
    output_shape: [68, 2]

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true # needed to set params
    lr: 0.001
    weight_decay: 0.0

  scheduler: 
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true # needed to set optimizer
    mode: min
    factor: 0.1
    patience: 10

logger:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  # name: "" # name of the run (normally generated by wandb)
  save_dir: "${paths.output_dir}"
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: "facial_landmarks"
  log_model: False # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  # entity: "" # set to name of your wandb team
  group: ""
  tags: []
  job_type: ""

trainer:
  _target_: lightning.pytorch.trainer.Trainer

  default_root_dir: ${paths.output_dir}

  min_epochs: 1 # prevents early stopping
  max_epochs: 100

  accelerator: gpu
  devices: 1

  # mixed precision for extra speed-up
  # precision: 16

  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1

  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False

paths:
  # path to root directory
  # this requires PROJECT_ROOT environment variable to exist
  # you can replace it with "." if you want the root to be the current working directory
  root_dir: ${oc.env:PROJECT_ROOT}

  # path to data directory
  data_dir: ${paths.root_dir}/data/

  # path to logging directory
  log_dir: ${paths.root_dir}/logs/

  # path to output directory, created dynamically by hydra
  # path generation pattern is specified in `configs/hydra/default.yaml`
  # use it to store all files generated during the run, like ckpts and metrics
  output_dir: ${hydra:runtime.output_dir}

  # path to working directory
  work_dir: ${hydra:runtime.cwd}

extras:
  # disable python warnings if they annoy you
  ignore_warnings: False

  # ask user for tags if none are provided in the config
  enforce_tags: True

  # pretty print config tree at the start of the run using Rich library
  print_config: True

hydra:
  # https://hydra.cc/docs/configure_hydra/intro/

  # enable color logging
  defaults:
    - override hydra_logging: colorlog
    - override job_logging: colorlog

  # output directory, generated dynamically on each run
  run:
    dir: ${paths.log_dir}/${task_name}/runs/${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ${paths.log_dir}/${task_name}/multiruns/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.num}



# task name, determines output directory path
task_name: "train"

# tags to help you identify your experiments
# you can overwrite this in experiment configs
# overwrite from command line with `python train.py tags="[first_tag, second_tag]"`
tags: ["dev"]

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# compile model for faster training with pytorch 2.0
compile: False

# simply provide checkpoint path to resume training
ckpt_path: null

# seed for random number generators in pytorch, numpy and python.random
seed: null
